{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[cuda(id=0), cuda(id=1), cuda(id=2), cuda(id=3)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "from jax.random import PRNGKey\n",
    "from jax import Array\n",
    "import os\n",
    "jax.devices() # Should be cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'probjax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tuple, List, Optional\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Some small helper functions\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprobjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transformer\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprobjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GaussianFourierEmbedding\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprobjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_fn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m denoising_score_matching_loss\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'probjax'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from functools import partial\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "# Some small helper functions\n",
    "from probjax.nn.transformers import Transformer\n",
    "from probjax.nn.helpers import GaussianFourierEmbedding\n",
    "from probjax.nn.loss_fn import denoising_score_matching_loss\n",
    "from probjax.distributions.sde import VESDE\n",
    "from probjax.distributions import Empirical, Independent\n",
    "\n",
    "import haiku as hk # Neural network library\n",
    "import optax # Gradient-based optimization in JAX\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('data/x.npy')\n",
    "theta = np.load('data/theta.npy')\n",
    "\n",
    "data =jnp.asarray(np.concatenate([theta, x], axis=1).reshape(len(x), -1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(key: PRNGKey, n:int):\n",
    "    key1, key2, key3 = jrandom.split(key,3)\n",
    "    theta1 = jrandom.normal(key1, (n, 1))  * 3 # Some prior on a parameter\n",
    "    x1 = 2*jnp.sin(theta1) + jrandom.normal(key2, (n, 1)) * 0.5 # Some data generated from the parameter \n",
    "    x2 = 0.1*theta1**2 + 0.5*jnp.abs(x1)*jrandom.normal(key3, (n, 1)) # Some data generated from the parameter\n",
    "    return jnp.concatenate([theta1,x1, x2], axis=1).reshape(n, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Setting up the diffusion process\n",
    "\n",
    "We will use the VESDE i.e. the variance exploding stochastic differential equation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_max = data.shape[1]\n",
    "node_ids = jnp.arange(nodes_max)\n",
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VESDE \n",
    "T = 1.\n",
    "T_min = 1e-2\n",
    "sigma_min = 1e-3\n",
    "sigma_max = 15.\n",
    "\n",
    "p0 = Independent(Empirical(data), 1) # Empirical distribution of the data\n",
    "sde = VESDE(p0, sigma_min=sigma_min , sigma_max=sigma_max)\n",
    "\n",
    "# Scaling fn for the output of the score model\n",
    "def output_scale_fn(t, x):\n",
    "    scale = jnp.clip(sde.marginal_stddev(t, jnp.ones_like(x)), 1e-2, None)\n",
    "    return (1/scale * x).reshape(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_value = 20      # Size of the value embedding\n",
    "dim_id = 20         # Size of the node id embedding\n",
    "dim_condition = 10  # Size of the condition embedding\n",
    "\n",
    "\n",
    "def model(t: Array, x: Array, node_ids: Array, condition_mask:Array, edge_mask: Optional[Array]=None):\n",
    "    \"\"\"Simplified Simformer model.\n",
    "\n",
    "    Args:\n",
    "        t (Array): Diffusion time\n",
    "        x (Array): Value of the nodes\n",
    "        node_ids (Array): Id of the nodes\n",
    "        condition_mask (Array): Condition state of the nodes\n",
    "        edge_mask (Array, optional): Edge mask. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Array: Score estimate of p(x_t)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, _ = x.shape\n",
    "    condition_mask = condition_mask.astype(jnp.bool_).reshape(-1,seq_len, 1)\n",
    "    node_ids = node_ids.reshape(-1,seq_len)\n",
    "    t = t.reshape(-1,1, 1)\n",
    "    \n",
    "    # Diffusion time embedding net (here we use a Gaussian Fourier embedding)\n",
    "    embedding_time = GaussianFourierEmbedding(64)  # Time embedding method\n",
    "    time_embeddings = embedding_time(t)\n",
    "    \n",
    "    # Tokinization part --------------------------------------------------------------------------------\n",
    "\n",
    "    embedding_net_value = lambda x: jnp.repeat(x, dim_value, axis=-1)    # Value embedding net (here we just repeat the value)\n",
    "    embedding_net_id = hk.Embed(nodes_max, dim_id, w_init=hk.initializers.RandomNormal(stddev=3.))   # Node id embedding nets (here we use a learnable random embedding vector)\n",
    "    condition_embedding = hk.get_parameter(\"condition_embedding\", shape=(1,1,dim_condition), init=hk.initializers.RandomNormal(stddev=0.5)) # Condition embedding (here we use a learnable random embedding vector)\n",
    "    condition_embedding = condition_embedding * condition_mask # If condition_mask is 0, then the embedding is 0, otherwise it is the condition_embedding vector\n",
    "    condition_embedding = jnp.broadcast_to(condition_embedding, (batch_size, seq_len, dim_condition))\n",
    "    \n",
    "    # Embed inputs and broadcast\n",
    "    value_embeddings = embedding_net_value(x)\n",
    "    id_embeddings = embedding_net_id(node_ids)\n",
    "    value_embeddings, id_embeddings = jnp.broadcast_arrays(value_embeddings, id_embeddings)\n",
    "    \n",
    "    # Concatenate embeddings (alternatively you can also add instead of concatenating)\n",
    "    x_encoded = jnp.concatenate([value_embeddings, id_embeddings, condition_embedding], axis=-1)\n",
    "    \n",
    "    # Transformer part --------------------------------------------------------------------------------\n",
    "    model = Transformer(num_heads=2, num_layers=2, attn_size=10, widening_factor=3) \n",
    "    \n",
    "    # Encode - here we just use a transformer to transform the tokenized inputs into a latent representation\n",
    "    h = model(x_encoded, context=time_embeddings, mask=edge_mask)\n",
    "\n",
    "    # Decode - here we just use a linear layer to get the score estimate (we scale the output by the marginal std dev)\n",
    "    out = hk.Linear(1)(h)\n",
    "    out = output_scale_fn(t, out) # SDE dependent output scaling\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Haiku, we need to initialize the model first, before we can use it.\n",
    "init, model_fn = hk.without_apply_rng(hk.transform(model)) # Init function initializes the parameters of the model, model_fn is the actual model function (which takes the parameters as first argument, hence is a \"pure function\")\n",
    "params = init(key, jnp.ones(data.shape[0]), data, node_ids, jnp.zeros_like(node_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters:  45994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'embed': {'embeddings': (14, 20)},\n",
       " 'gaussian_fourier_embedding': {'B': (33, 1)},\n",
       " 'linear': {'b': (1,), 'w': (50, 1)},\n",
       " 'transformer/layer_norm': {'offset': (50,), 'scale': (50,)},\n",
       " 'transformer/layer_norm_1': {'offset': (50,), 'scale': (50,)},\n",
       " 'transformer/layer_norm_2': {'offset': (50,), 'scale': (50,)},\n",
       " 'transformer/layer_norm_3': {'offset': (50,), 'scale': (50,)},\n",
       " 'transformer/layer_norm_4': {'offset': (50,), 'scale': (50,)},\n",
       " 'transformer/linear': {'b': (150,), 'w': (50, 150)},\n",
       " 'transformer/linear_1': {'b': (50,), 'w': (150, 50)},\n",
       " 'transformer/linear_2': {'b': (50,), 'w': (64, 50)},\n",
       " 'transformer/linear_3': {'b': (150,), 'w': (50, 150)},\n",
       " 'transformer/linear_4': {'b': (50,), 'w': (150, 50)},\n",
       " 'transformer/linear_5': {'b': (50,), 'w': (64, 50)},\n",
       " 'transformer/multi_head_attention/key': {'b': (20,), 'w': (50, 20)},\n",
       " 'transformer/multi_head_attention/linear': {'b': (50,), 'w': (20, 50)},\n",
       " 'transformer/multi_head_attention/query': {'b': (20,), 'w': (50, 20)},\n",
       " 'transformer/multi_head_attention/value': {'b': (20,), 'w': (50, 20)},\n",
       " 'transformer/multi_head_attention_1/key': {'b': (20,), 'w': (50, 20)},\n",
       " 'transformer/multi_head_attention_1/linear': {'b': (50,), 'w': (20, 50)},\n",
       " 'transformer/multi_head_attention_1/query': {'b': (20,), 'w': (50, 20)},\n",
       " 'transformer/multi_head_attention_1/value': {'b': (20,), 'w': (50, 20)},\n",
       " '~': {'condition_embedding': (1, 1, 10)}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we can see the total number of parameters and their shapes\n",
    "print(\"Total number of parameters: \", jax.tree_util.tree_reduce(lambda x,y: x+y, jax.tree_map(lambda x: x.size, params)))\n",
    "jax.tree_util.tree_map(lambda x: x.shape, params) # Here we can see the shapes of the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) The loss\n",
    "Here we will show the variant which targets to learn:\n",
    "\n",
    "* Correct joint $p(\\theta,x_1, x_2)$\n",
    "* Correct conditionals $p(\\theta|x), p(x|\\theta), ...$\n",
    "* Correct marginals $p(\\theta), p(x), ...$\n",
    "    \n",
    "Base loss is an **denoising score matching objective**:\n",
    "$$ \\mathcal{L}(\\phi) = \\mathbb{E}_{t \\sim Unif(0,1)} \\left[ \\lambda(t) \\mathbb{E}_{x_0, x_t \\sim p(x_0)p(x_t|x_0)}\\left[ || s_\\phi(x_t, t) - \\nabla_{x_t} \\log p(x_t|x_0)||_2^2 \\right] \\right] $$\n",
    "all the different *targets* will be implemented through masking out different things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_fn(t:Array):\n",
    "    # MLE weighting\n",
    "    return jnp.clip(sde.diffusion(t, jnp.ones((1,1,1)))**2, 1e-4)\n",
    "\n",
    "def marginalize(rng: PRNGKey, edge_mask: Array):\n",
    "    # Simple function that marginializes out a single node from a adjacency matrix of a graph.\n",
    "    idx = jax.random.choice(rng, jnp.arange(edge_mask.shape[0]), shape=(1,), replace=False)\n",
    "    edge_mask = edge_mask.at[idx, :].set(False)\n",
    "    edge_mask = edge_mask.at[:, idx].set(False)\n",
    "    edge_mask = edge_mask.at[idx, idx].set(True)\n",
    "    return edge_mask\n",
    "\n",
    "def loss_fn(params: dict, key: PRNGKey, batch_size:int= 1024, data: Array = data):\n",
    "\n",
    "    rng_time, rng_sample, rng_data, rng_condition, rng_edge_mask1, rng_edge_mask2 = jax.random.split(key, 6)\n",
    "    \n",
    "    # Generate data and random times\n",
    "    times = jax.random.uniform(rng_time, (batch_size, 1, 1), minval=T_min, maxval=1.0)\n",
    "    batch_xs = data\n",
    "\n",
    "    # Node ids (can be subsampled but here we use all nodes)\n",
    "    ids = node_ids\n",
    "    \n",
    "\n",
    "    # Condition mask -> randomly condition on some data.\n",
    "    condition_mask = jax.random.bernoulli(rng_condition, 0.333, shape=(batch_xs.shape[0], batch_xs.shape[1]))\n",
    "    condition_mask_all_one = jnp.all(condition_mask, axis=-1, keepdims=True)\n",
    "    condition_mask *= condition_mask_all_one # Avoid conditioning on all nodes -> nothing to train...\n",
    "    condition_mask = condition_mask[..., None]\n",
    "    # Alternatively you can also set the condition mask manually to specific conditional distributions.\n",
    "    # condition_mask = jnp.zeros((3,), dtype=jnp.bool_)  # Joint mask\n",
    "    # condition_mask = jnp.array([False, True, True], dtype=jnp.bool_)  # Posterior mask\n",
    "    # condition_mask = jnp.array([True, False, False], dtype=jnp.bool_)  # Likelihod mask\n",
    "    \n",
    "    # You can also structure the base mask!\n",
    "    edge_mask = jnp.ones((4*batch_size//5, batch_xs.shape[1],batch_xs.shape[1]), dtype=jnp.bool_) # Dense default mask \n",
    "    \n",
    "    # Optional: Include marginal consistency\n",
    "    marginal_mask = jax.vmap(marginalize, in_axes=(0,None))(jax.random.split(rng_edge_mask1, (batch_size//5,)), edge_mask[0])\n",
    "    edge_masks = jnp.concatenate([edge_mask, marginal_mask], axis=0)\n",
    "    edge_masks = jax.random.choice(rng_edge_mask2, edge_masks, shape=(batch_size,), axis=0) # Randomly choose between dense and marginal mask\n",
    "    \n",
    "\n",
    "    # Forward diffusion, do not perturb conditioned data\n",
    "    # Will use the condition mask to mask to prevent adding noise for nodes that are conditioned.\n",
    "    loss = denoising_score_matching_loss(params, rng_sample, times, batch_xs, condition_mask, model_fn= model_fn, mean_fn=sde.marginal_mean, std_fn = sde.marginal_stddev, weight_fn=weight_fn,node_ids=ids, condition_mask=condition_mask, edge_mask=edge_masks)\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Training\n",
    "\n",
    "Simple training loop (compatible with multiple GPUs, TPUs). Here simpy optimizing with Adam for a fixed amount of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(1e-3)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "@partial(jax.jit, static_argnums=(4,)) #@partial(jax.pmap, axis_name=\"num_devices\")\n",
    "def update(params, rng, opt_state, data, batch_size):\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params, rng, batch_size, data)\n",
    "    \n",
    "    loss = jax.lax.pmean(loss, axis_name=\"num_devices\")\n",
    "    grads = jax.lax.pmean(grads, axis_name=\"num_devices\")\n",
    "    \n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params=params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return loss, params, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_devices = 1 #jax.local_device_count()\n",
    "replicated_params = jax.tree_map(lambda x: jnp.array([x] * n_devices), params)\n",
    "replicated_opt_state = jax.tree_map(lambda x: jnp.array([x] * n_devices), opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'gaussian_fourier_embedding/B' with retrieved shape (1, 33, 1) does not match shape=[33, 1] dtype=<class 'jax.numpy.float32'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     batch_data \u001b[38;5;241m=\u001b[39m data_epoch[i\u001b[38;5;241m*\u001b[39mbatch_size:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mbatch_size]\n\u001b[1;32m     10\u001b[0m     key, subkey \u001b[38;5;241m=\u001b[39m jrandom\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[0;32m---> 11\u001b[0m     loss, replicated_params, replicated_opt_state \u001b[38;5;241m=\u001b[39m \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicated_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplicated_opt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     l \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39mbatch_size\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(l)\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m, in \u001b[0;36mupdate\u001b[0;34m(params, rng, opt_state, data, batch_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;129m@partial\u001b[39m(jax\u001b[38;5;241m.\u001b[39mjit, static_argnums\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m,)) \u001b[38;5;66;03m#@partial(jax.pmap, axis_name=\"num_devices\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(params, rng, opt_state, data, batch_size):\n\u001b[0;32m----> 6\u001b[0m     loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mpmean(loss, axis_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_devices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m     grads \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mpmean(grads, axis_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_devices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[19], line 46\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(params, key, batch_size, data)\u001b[0m\n\u001b[1;32m     41\u001b[0m edge_masks \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(rng_edge_mask2, edge_masks, shape\u001b[38;5;241m=\u001b[39m(batch_size,), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# Randomly choose between dense and marginal mask\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Forward diffusion, do not perturb conditioned data\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Will use the condition mask to mask to prevent adding noise for nodes that are conditioned.\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mdenoising_score_matching_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_xs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msde\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarginal_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msde\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarginal_stddev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnode_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcondition_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/sbi_chemical_abundances/simformer/src/probjax/probjax/nn/loss_fn.py:135\u001b[0m, in \u001b[0;36mdenoising_score_matching_loss\u001b[0;34m(params, key, times, xs_target, loss_mask, model_fn, mean_fn, std_fn, weight_fn, axis, rebalance_loss, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m     loss_mask \u001b[38;5;241m=\u001b[39m loss_mask\u001b[38;5;241m.\u001b[39mreshape(xs_target\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    133\u001b[0m     xs_t \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mwhere(loss_mask, xs_target, xs_t)\n\u001b[0;32m--> 135\u001b[0m score_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m score_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39meps \u001b[38;5;241m/\u001b[39m std_t\n\u001b[1;32m    138\u001b[0m loss \u001b[38;5;241m=\u001b[39m (score_pred \u001b[38;5;241m-\u001b[39m score_target) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/simformer/lib/python3.10/site-packages/haiku/_src/multi_transform.py:314\u001b[0m, in \u001b[0;36mwithout_apply_rng.<locals>.apply_fn\u001b[0;34m(params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_fn\u001b[39m(params, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m   check_rng_kwarg(kwargs)\n\u001b[0;32m--> 314\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/simformer/lib/python3.10/site-packages/haiku/_src/transform.py:183\u001b[0m, in \u001b[0;36mwithout_state.<locals>.apply_fn\u001b[0;34m(params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    177\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    178\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHaiku transform adds three arguments (params, state, rng) to apply. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    179\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf the functions you are transforming use the same names you must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass them positionally (e.g. `f.apply(.., my_state)` and not by \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname (e.g. `f.apply(.., state=my_state)`)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 183\u001b[0m out, state \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state:\n\u001b[1;32m    185\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m base\u001b[38;5;241m.\u001b[39mNonEmptyStateError(\n\u001b[1;32m    186\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf your transformed function uses `hk.\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mget,set}_state` then use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    187\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`hk.transform_with_state`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/simformer/lib/python3.10/site-packages/haiku/_src/transform.py:456\u001b[0m, in \u001b[0;36mtransform_with_state.<locals>.apply_fn\u001b[0;34m(params, state, rng, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m base\u001b[38;5;241m.\u001b[39mnew_context(params\u001b[38;5;241m=\u001b[39mparams, state\u001b[38;5;241m=\u001b[39mstate, rng\u001b[38;5;241m=\u001b[39mrng) \u001b[38;5;28;01mas\u001b[39;00m ctx:\n\u001b[1;32m    455\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 456\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m jax\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mUnexpectedTracerError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m jax\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mUnexpectedTracerError(unexpected_tracer_hint) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 26\u001b[0m, in \u001b[0;36mmodel\u001b[0;34m(t, x, node_ids, condition_mask, edge_mask)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Diffusion time embedding net (here we use a Gaussian Fourier embedding)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m embedding_time \u001b[38;5;241m=\u001b[39m GaussianFourierEmbedding(\u001b[38;5;241m64\u001b[39m)  \u001b[38;5;66;03m# Time embedding method\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m time_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Tokinization part --------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     30\u001b[0m embedding_net_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: jnp\u001b[38;5;241m.\u001b[39mrepeat(x, dim_value, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)    \u001b[38;5;66;03m# Value embedding net (here we just repeat the value)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/simformer/lib/python3.10/site-packages/haiku/_src/module.py:458\u001b[0m, in \u001b[0;36mwrap_method.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m method_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    456\u001b[0m     f \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnamed_call(f, name\u001b[38;5;241m=\u001b[39mmethod_name)\n\u001b[0;32m--> 458\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Module names are set in the constructor. If `f` is the constructor then\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;66;03m# its name will only be set **after** `f` has run. For methods other\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# than `__init__` we need the name before running in order to wrap their\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# execution with `named_call`.\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/simformer/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/simformer/lib/python3.10/site-packages/haiku/_src/module.py:299\u001b[0m, in \u001b[0;36mrun_interceptors\u001b[0;34m(bound_method, method_name, self, orig_class, *args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs any method interceptors or the original method.\"\"\"\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interceptor_stack:\n\u001b[0;32m--> 299\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m ctx \u001b[38;5;241m=\u001b[39m MethodContext(module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    302\u001b[0m                     method_name\u001b[38;5;241m=\u001b[39mmethod_name,\n\u001b[1;32m    303\u001b[0m                     orig_method\u001b[38;5;241m=\u001b[39mbound_method,\n\u001b[1;32m    304\u001b[0m                     orig_class\u001b[38;5;241m=\u001b[39morig_class)\n\u001b[1;32m    305\u001b[0m interceptor_stack_copy \u001b[38;5;241m=\u001b[39m interceptor_stack\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[0;32m~/sbi_chemical_abundances/simformer/src/probjax/probjax/nn/helpers.py:109\u001b[0m, in \u001b[0;36mGaussianFourierEmbedding.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m    108\u001b[0m     half_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 109\u001b[0m     B \u001b[38;5;241m=\u001b[39m \u001b[43mhk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parameter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mhalf_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRandomNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearnable:\n\u001b[1;32m    113\u001b[0m         B \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mstop_gradient(B)\n",
      "File \u001b[0;32m~/anaconda3/envs/simformer/lib/python3.10/site-packages/haiku/_src/base.py:685\u001b[0m, in \u001b[0;36mget_parameter\u001b[0;34m(name, shape, dtype, init)\u001b[0m\n\u001b[1;32m    682\u001b[0m param \u001b[38;5;241m=\u001b[39m check_not_none(param, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameters cannot be `None`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(shape):\n\u001b[0;32m--> 685\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    686\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfq_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m with retrieved shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m does not match \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    687\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m param\n",
      "\u001b[0;31mValueError\u001b[0m: 'gaussian_fourier_embedding/B' with retrieved shape (1, 33, 1) does not match shape=[33, 1] dtype=<class 'jax.numpy.float32'>"
     ]
    }
   ],
   "source": [
    "key = jrandom.PRNGKey(0)\n",
    "for _ in range(10):\n",
    "    batch_size = 1024\n",
    "    l = 0\n",
    "    key, subkey = jrandom.split(key)\n",
    "    data_epoch = jax.random.permutation(key, data)\n",
    "\n",
    "    for i in range(len(data_epoch)//batch_size):\n",
    "        batch_data = data_epoch[i*batch_size:(i+1)*batch_size]\n",
    "        key, subkey = jrandom.split(key)\n",
    "        loss, replicated_params, replicated_opt_state = update(replicated_params, subkey, replicated_opt_state, batch_size=batch_size, data=batch_data)\n",
    "        l += loss[0] /batch_size\n",
    "    print(l)\n",
    "params = jax.tree_map(lambda x: x[0], replicated_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = jax.tree_map(lambda x: x[0], replicated_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Sampling from the joint and the marginals\n",
    "\n",
    "For this we will implement a simple SDE-based sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from probjax.utils.sdeint import sdeint\n",
    "\n",
    "condition_mask = jnp.zeros((nodes_max,))\n",
    "condition_value = jnp.zeros((nodes_max,))\n",
    "\n",
    "# Reverse SDE drift\n",
    "def drift_backward(t, x, node_ids=node_ids, condition_mask=condition_mask, edge_mask=None, score_fn = model_fn, replace_conditioned=True):\n",
    "    score = score_fn(params, t.reshape(-1, 1, 1), x.reshape(-1, len(node_ids), 1), node_ids,condition_mask[:len(node_ids)], edge_mask=edge_mask)\n",
    "    score = score.reshape(x.shape)\n",
    "\n",
    "    f =  sde.drift(t,x) - sde.diffusion(t,x)**2 * score\n",
    "    if replace_conditioned:\n",
    "        f = f * (1-condition_mask[:len(node_ids)])\n",
    "    \n",
    "    return f\n",
    "\n",
    "# Reverse SDE diffusion\n",
    "def diffusion_backward(t,x, node_ids=node_ids,condition_mask=condition_mask, replace_conditioned=True):\n",
    "    b =  sde.diffusion(t,x) \n",
    "    if replace_conditioned:\n",
    "        b = b * (1-condition_mask[:len(node_ids)])\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "end_std = jnp.squeeze(sde.marginal_stddev(jnp.ones(1)))\n",
    "end_mean = jnp.squeeze(sde.marginal_mean(jnp.ones(1)))\n",
    "\n",
    "@partial(jax.jit, static_argnums=(1,3,7,8))\n",
    "def sample_fn(key, shape, node_ids=node_ids, time_steps=500, condition_mask=jnp.zeros((nodes_max,), dtype=int), condition_value=jnp.zeros((nodes_max,)), edge_mask=None, score_fn = model_fn, replace_conditioned=True):\n",
    "    condition_mask = condition_mask[:len(node_ids)]\n",
    "    key1, key2 = jrandom.split(key, 2)\n",
    "    # Sample from noise distribution at time 1\n",
    "    x_T = jax.random.normal(key1, shape + (len(node_ids),)) * end_std[node_ids] + end_mean[node_ids]\n",
    "    \n",
    "    if replace_conditioned:\n",
    "        x_T = x_T * (1-condition_mask) + condition_value * condition_mask\n",
    "    # Sove backward sde\n",
    "    keys = jrandom.split(key2, shape)\n",
    "    ys = jax.vmap(lambda *args: sdeint(*args, noise_type=\"diagonal\"), in_axes= (0, None, None, 0, None), out_axes=0)(keys, lambda t, x: drift_backward(t, x, node_ids, condition_mask, edge_mask=edge_mask, score_fn=score_fn, replace_conditioned=replace_conditioned), lambda t, x: diffusion_backward(t, x, node_ids, condition_mask, replace_conditioned=replace_conditioned), x_T, jnp.linspace(1.,T_min, time_steps))\n",
    "    return ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full joint estimation\n",
    "samples = sample_fn(jrandom.PRNGKey(0), (10000,), node_ids, condition_mask=jnp.zeros((nodes_max,), dtype=int), condition_value=jnp.zeros((nodes_max,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = jnp.load(\"samples.npy\")\n",
    "\n",
    "s = np.zeros((len(samples[0]),), dtype=object)\n",
    "for i in range (len(samples[0])):\n",
    "    s[i] = pd.DataFrame(samples[:,i,:], columns=labels_in+labels_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3, figsize=(15,10))\n",
    "lin = np.linspace(-20, 20, 1000)\n",
    "\n",
    "def update_hist(j):\n",
    "    plt.suptitle(f\"Step: {j}\")\n",
    "    for i in range(6):\n",
    "        if i < 5:\n",
    "            ax[i//3, i%3].cla()\n",
    "            \n",
    "            x_data = samples[:,j, i]\n",
    "            x_mu = x_data.mean()\n",
    "            x_std = x_data.std()\n",
    "            y = norm.pdf(lin, x_mu, x_std)\n",
    "\n",
    "            ax[i//3, i%3].plot(lin, y, color=\"#502933\", lw=1)\n",
    "            ax[i//3, i%3].fill_between(lin, y, color=\"#502933\", alpha=0.5)\n",
    "            ax[i//3, i%3].vlines(priors[i][0], 0,1, colors=\"k\", linestyle=\"--\")\n",
    "\n",
    "            #ax[i//3, i%3].hist(samples[:,j*10, i], bins=1000, density=True, color=\"black\")\n",
    "            ax[i//3, i%3].set_xlim(-20, 20)\n",
    "            ax[i//3, i%3].set_ylim(0, 1)\n",
    "            ax[i//3, i%3].set_title(labels_in[i])\n",
    "\n",
    "        elif i == 5:\n",
    "            ax[i//3, i%3].cla()\n",
    "            ax[i//3, i%3].hist(samples[:,j, i], bins=1000, density=True, color=\"#502933\")\n",
    "            ax[i//3, i%3].set_xlim(-20, 20)\n",
    "            ax[i//3, i%3].set_ylim(0, 0.5)\n",
    "            ax[i//3, i%3].hlines(0.1, 0, 12.8, colors=\"k\", linestyle=\"--\")\n",
    "            ax[i//3, i%3].set_title(labels_in[i])\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update_hist, frames=300, interval=80)\n",
    "ani.save(filename=\"test.gif\", writer=\"pillow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    z = pd.DataFrame(samples[:,i*2,:], columns=labels_in+labels_out)\n",
    "    plot = sns.pairplot(z, kind=\"kde\")\n",
    "\n",
    "    for j in range(5):\n",
    "        plot.axes[j, j].set_xlim(-5, 5)\n",
    "        plot.axes[j, j].set_ylim(-5, 5)\n",
    "    plot.axes[5, 5].set_xlim(-5, 15)\n",
    "    for j in range(6,14):\n",
    "        plot.axes[j,j].set_xlim(-2,2)\n",
    "        plot.axes[j,j].set_ylim(-2,2)\n",
    "\n",
    "    plot.savefig(f\"sns/pairplot_{i}.png\")\n",
    "    plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
