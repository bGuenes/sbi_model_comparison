{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "from jax.random import PRNGKey\n",
    "from jax import Array\n",
    "import os\n",
    "jax.devices() # Should be cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from functools import partial\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "import haiku as hk # Neural network library\n",
    "import optax # Gradient-based optimization in JAX\n",
    "\n",
    "# Some small helper functions\n",
    "from probjax.nn.transformers import Transformer\n",
    "from probjax.nn.helpers import GaussianFourierEmbedding\n",
    "from probjax.nn.loss_fn import denoising_score_matching_loss\n",
    "from probjax.distributions.sde import VESDE\n",
    "from probjax.distributions import Empirical, Independent\n",
    "\n",
    "from sbi.analysis import pairplot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random key\n",
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1) Setting up a toy problem\n",
    "\n",
    "This is a simple toy problem to test the functionality of the `Simformer` method.\n",
    "$$ \\theta \\sim \\mathcal{N}(\\theta; 0, 3^2) \\qquad \\qquad  x_1 \\sim \\mathcal{N}(x_1; 2\\cdot\\sin(\\theta), 0.5^2)  \\qquad \\qquad   x_2 \\sim \\mathcal{N}(x_2;0.1\\cdot \\theta^2,  (0.5\\cdot |x_1|)^2)$$\n",
    "\n",
    "Here we have 3 nonlinear related Gaussian variables. The associated joint distirbution is visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def generate_data(key: PRNGKey, n:int):\n",
    "    key1, key2, key3 = jrandom.split(key,3)\n",
    "    theta1 = jrandom.normal(key1, (n, 1))  * 3 # Some prior on a parameter\n",
    "    x1 = 2*jnp.sin(theta1) + jrandom.normal(key2, (n, 1)) * 0.5 # Some data generated from the parameter \n",
    "    x2 = 0.1*theta1**2 + 0.5*jnp.abs(x1)*jrandom.normal(key3, (n, 1)) # Some data generated from the parameter\n",
    "    return jnp.concatenate([theta1,x1, x2], axis=1).reshape(n, -1, 1)\n",
    "\n",
    "def log_potential(theta1: Array, x1: Array, x2: Array, sigma_x1:float=0.5, sigma_x2:float=0.5, mean_loc:float=0.0, mean_scale:float=3.0 ):\n",
    "    log_prob_theta = jax.scipy.stats.norm.logpdf(theta1, mean_loc, mean_scale)\n",
    "    if x1 is not None:\n",
    "        log_prob_x1 = jax.scipy.stats.norm.logpdf(x1, 2*jnp.sin(theta1), sigma_x1)\n",
    "    else:\n",
    "        log_prob_x1 = 0\n",
    "    if x2 is not None:\n",
    "        log_prob_x2 = jax.scipy.stats.norm.logpdf(x2, 0.1*theta1**2,  sigma_x2*jnp.abs(x1))\n",
    "    else:\n",
    "        log_prob_x2 = 0\n",
    "    \n",
    "    return log_prob_theta + log_prob_x1 + log_prob_x2\n",
    "    \n",
    "\n",
    "data = generate_data(jrandom.PRNGKey(1), 10000)  # Shape: (n, nodes, dim) here dim = 1\n",
    "nodes_max = data.shape[1]\n",
    "node_ids = jnp.arange(nodes_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "_ = pairplot(np.array(data[...,0]), labels=[\"theta\", \"x1\", \"x2\"], figsize=(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2) Setting up the diffusion process\n",
    "\n",
    "We will use the VESDE i.e. the variance exploding stochastic differential equation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# VESDE \n",
    "T = 1.\n",
    "T_min = 1e-2\n",
    "sigma_min = 1e-3\n",
    "sigma_max = 15.\n",
    "\n",
    "p0 = Independent(Empirical(data), 1) # Empirical distribution of the data\n",
    "sde = VESDE(p0, sigma_min=sigma_min , sigma_max=sigma_max)\n",
    "\n",
    "# Scaling fn for the output of the score model\n",
    "def output_scale_fn(t, x):\n",
    "    scale = jnp.clip(sde.marginal_stddev(t, jnp.ones_like(x)), 1e-2, None)\n",
    "    return (1/scale * x).reshape(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sde.marginal_stddev(10, jnp.ones_like(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3) Building the Simformer\n",
    "\n",
    "This can be divided into two parts, each offering various choices:\n",
    "\n",
    "* **Tokenizer**: \n",
    "This component jointly embeds 'x', 'node_ids', and the 'condition_mask' into a unified \n",
    "vector known as a token.\n",
    "    * **Value Embedding**: Embeds the value of the variable.\n",
    "    * **Node Embedding**: Embeds the node ID.\n",
    "    * **Condition Embedding**: Embeds the condition mask.\n",
    "* **Transformer**: This is a transformer model that takes tokens as input and generates scores for each node.\n",
    "    * **Num_heads**: Specifies the number of heads in the multi-head attention mechanism.\n",
    "    * **Attn_size**: Determines the size of attention, i.e., the dimensions to which query and key are projected.\n",
    "    * **Num_layers**: Sets the number of layers in the transformer.\n",
    "    * **Widening_factor**: Specifies the factor by which the hidden size of the MLP is increased in each layer.\n",
    "\n",
    "In this example, we construct a compact Simformer with 2 layers and 2 heads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dim_value = 20      # Size of the value embedding\n",
    "dim_id = 20         # Size of the node id embedding\n",
    "dim_condition = 10  # Size of the condition embedding\n",
    "\n",
    "\n",
    "def model(t: Array, x: Array, node_ids: Array, condition_mask:Array, edge_mask: Optional[Array]=None):\n",
    "    \"\"\"Simplified Simformer model.\n",
    "\n",
    "    Args:\n",
    "        t (Array): Diffusion time\n",
    "        x (Array): Value of the nodes\n",
    "        node_ids (Array): Id of the nodes\n",
    "        condition_mask (Array): Condition state of the nodes\n",
    "        edge_mask (Array, optional): Edge mask. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Array: Score estimate of p(x_t)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, _ = x.shape\n",
    "    condition_mask = condition_mask.astype(jnp.bool_).reshape(-1,seq_len, 1)\n",
    "    node_ids = node_ids.reshape(-1,seq_len)\n",
    "    t = t.reshape(-1,1, 1)\n",
    "    \n",
    "    # Diffusion time embedding net (here we use a Gaussian Fourier embedding)\n",
    "    embedding_time = GaussianFourierEmbedding(64)  # Time embedding method\n",
    "    time_embeddings = embedding_time(t)\n",
    "    \n",
    "    # Tokinization part --------------------------------------------------------------------------------\n",
    "\n",
    "    embedding_net_value = lambda x: jnp.repeat(x, dim_value, axis=-1)    # Value embedding net (here we just repeat the value)\n",
    "    embedding_net_id = hk.Embed(nodes_max, dim_id, w_init=hk.initializers.RandomNormal(stddev=3.))   # Node id embedding nets (here we use a learnable random embedding vector)\n",
    "    condition_embedding = hk.get_parameter(\"condition_embedding\", shape=(1,1,dim_condition), init=hk.initializers.RandomNormal(stddev=0.5)) # Condition embedding (here we use a learnable random embedding vector)\n",
    "    condition_embedding = condition_embedding * condition_mask # If condition_mask is 0, then the embedding is 0, otherwise it is the condition_embedding vector\n",
    "    condition_embedding = jnp.broadcast_to(condition_embedding, (batch_size, seq_len, dim_condition))\n",
    "    \n",
    "    # Embed inputs and broadcast\n",
    "    value_embeddings = embedding_net_value(x)\n",
    "    id_embeddings = embedding_net_id(node_ids)\n",
    "    value_embeddings, id_embeddings = jnp.broadcast_arrays(value_embeddings, id_embeddings)\n",
    "    \n",
    "    # Concatenate embeddings (alternatively you can also add instead of concatenating)\n",
    "    x_encoded = jnp.concatenate([value_embeddings, id_embeddings, condition_embedding], axis=-1)\n",
    "    \n",
    "    # Transformer part --------------------------------------------------------------------------------\n",
    "    model = Transformer(num_heads=2, num_layers=2, attn_size=10, widening_factor=3) \n",
    "    \n",
    "    # Encode - here we just use a transformer to transform the tokenized inputs into a latent representation\n",
    "    h = model(x_encoded, context=time_embeddings, mask=edge_mask)\n",
    "\n",
    "    # Decode - here we just use a linear layer to get the score estimate (we scale the output by the marginal std dev)\n",
    "    out = hk.Linear(1)(h)\n",
    "    out = output_scale_fn(t, out) # SDE dependent output scaling\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# In Haiku, we need to initialize the model first, before we can use it.\n",
    "init, model_fn = hk.without_apply_rng(hk.transform(model)) # Init function initializes the parameters of the model, model_fn is the actual model function (which takes the parameters as first argument, hence is a \"pure function\")\n",
    "params = init(key, jnp.ones(data.shape[0]), data, node_ids, jnp.zeros_like(node_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Here we can see the total number of parameters and their shapes\n",
    "print(\"Total number of parameters: \", jax.tree_util.tree_reduce(lambda x,y: x+y, jax.tree_map(lambda x: x.size, params)))\n",
    "jax.tree_util.tree_map(lambda x: x.shape, params) # Here we can see the shapes of the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4) The loss\n",
    "Here we will show the variant which targets to learn:\n",
    "\n",
    "* Correct joint $p(\\theta,x_1, x_2)$\n",
    "* Correct conditionals $p(\\theta|x), p(x|\\theta), ...$\n",
    "* Correct marginals $p(\\theta), p(x), ...$\n",
    "    \n",
    "Base loss is an **denoising score matching objective**:\n",
    "$$ \\mathcal{L}(\\phi) = \\mathbb{E}_{t \\sim Unif(0,1)} \\left[ \\lambda(t) \\mathbb{E}_{x_0, x_t \\sim p(x_0)p(x_t|x_0)}\\left[ || s_\\phi(x_t, t) - \\nabla_{x_t} \\log p(x_t|x_0)||_2^2 \\right] \\right] $$\n",
    "all the different *targets* will be implemented through masking out different things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def weight_fn(t:Array):\n",
    "    # MLE weighting\n",
    "    return jnp.clip(sde.diffusion(t, jnp.ones((1,1,1)))**2, 1e-4)\n",
    "\n",
    "def marginalize(rng: PRNGKey, edge_mask: Array):\n",
    "    # Simple function that marginializes out a single node from a adjacency matrix of a graph.\n",
    "    idx = jax.random.choice(rng, jnp.arange(edge_mask.shape[0]), shape=(1,), replace=False)\n",
    "    edge_mask = edge_mask.at[idx, :].set(False)\n",
    "    edge_mask = edge_mask.at[:, idx].set(False)\n",
    "    edge_mask = edge_mask.at[idx, idx].set(True)\n",
    "    return edge_mask\n",
    "\n",
    "def loss_fn(params: dict, key: PRNGKey, batch_size:int= 1024):\n",
    "\n",
    "    rng_time, rng_sample, rng_data, rng_condition, rng_edge_mask1, rng_edge_mask2 = jax.random.split(key, 6)\n",
    "    \n",
    "    # Generate data and random times\n",
    "    times = jax.random.uniform(rng_time, (batch_size, 1, 1), minval=T_min, maxval=1.0)\n",
    "    batch_xs = generate_data(rng_data, batch_size) # n, T_max, 1\n",
    "\n",
    "    # Node ids (can be subsampled but here we use all nodes)\n",
    "    ids = node_ids\n",
    "    \n",
    "\n",
    "    # Condition mask -> randomly condition on some data.\n",
    "    condition_mask = jax.random.bernoulli(rng_condition, 0.333, shape=(batch_xs.shape[0], batch_xs.shape[1]))\n",
    "    condition_mask_all_one = jnp.all(condition_mask, axis=-1, keepdims=True)\n",
    "    condition_mask *= condition_mask_all_one # Avoid conditioning on all nodes -> nothing to train...\n",
    "    condition_mask = condition_mask[..., None]\n",
    "    # Alternatively you can also set the condition mask manually to specific conditional distributions.\n",
    "    # condition_mask = jnp.zeros((3,), dtype=jnp.bool_)  # Joint mask\n",
    "    # condition_mask = jnp.array([False, True, True], dtype=jnp.bool_)  # Posterior mask\n",
    "    # condition_mask = jnp.array([True, False, False], dtype=jnp.bool_)  # Likelihod mask\n",
    "    \n",
    "    # You can also structure the base mask!\n",
    "    edge_mask = jnp.ones((4*batch_size//5, batch_xs.shape[1],batch_xs.shape[1]), dtype=jnp.bool_) # Dense default mask \n",
    "    \n",
    "    # Optional: Include marginal consistency\n",
    "    marginal_mask = jax.vmap(marginalize, in_axes=(0,None))(jax.random.split(rng_edge_mask1, (batch_size//5,)), edge_mask[0])\n",
    "    edge_masks = jnp.concatenate([edge_mask, marginal_mask], axis=0)\n",
    "    edge_masks = jax.random.choice(rng_edge_mask2, edge_masks, shape=(batch_size,), axis=0) # Randomly choose between dense and marginal mask\n",
    "    \n",
    "\n",
    "    # Forward diffusion, do not perturb conditioned data\n",
    "    # Will use the condition mask to mask to prevent adding noise for nodes that are conditioned.\n",
    "    loss = denoising_score_matching_loss(params, rng_sample, times, batch_xs, condition_mask, model_fn= model_fn, mean_fn=sde.marginal_mean, std_fn = sde.marginal_stddev, weight_fn=weight_fn,node_ids=ids, condition_mask=condition_mask, edge_mask=edge_masks)\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5) Training\n",
    "\n",
    "Simple training loop (compatible with multiple GPUs, TPUs). Here simpy optimizing with Adam for a fixed amount of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optax.adam(1e-3)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "@partial(jax.pmap, axis_name=\"num_devices\")\n",
    "def update(params, rng, opt_state):\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params, rng)\n",
    "    \n",
    "    loss = jax.lax.pmean(loss, axis_name=\"num_devices\")\n",
    "    grads = jax.lax.pmean(grads, axis_name=\"num_devices\")\n",
    "    \n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params=params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return loss, params, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "n_devices = jax.local_device_count()\n",
    "replicated_params = jax.tree_map(lambda x: jnp.array([x] * n_devices), params)\n",
    "replicated_opt_state = jax.tree_map(lambda x: jnp.array([x] * n_devices), opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "key = jrandom.PRNGKey(0)\n",
    "for _ in range(15):\n",
    "    l = 0\n",
    "    for i in range(5000):\n",
    "        key, subkey = jrandom.split(key)\n",
    "        loss, replicated_params, replicated_opt_state = update(replicated_params, jax.random.split(subkey, (n_devices,)), replicated_opt_state)\n",
    "        l += loss[0] /5000\n",
    "    print(l)\n",
    "params = jax.tree_map(lambda x: x[0], replicated_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = jax.tree_map(lambda x: x[0], replicated_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 6) Sampling from the joint and the marginals\n",
    "\n",
    "For this we will implement a simple SDE-based sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from probjax.utils.sdeint import sdeint\n",
    "\n",
    "condition_mask = jnp.zeros((nodes_max,))\n",
    "condition_value = jnp.zeros((nodes_max,))\n",
    "\n",
    "# Reverse SDE drift\n",
    "def drift_backward(t, x, node_ids=node_ids, condition_mask=condition_mask, edge_mask=None, score_fn = model_fn, replace_conditioned=True):\n",
    "    score = score_fn(params, t.reshape(-1, 1, 1), x.reshape(-1, len(node_ids), 1), node_ids,condition_mask[:len(node_ids)], edge_mask=edge_mask)\n",
    "    score = score.reshape(x.shape)\n",
    "\n",
    "    f =  sde.drift(t,x) - sde.diffusion(t,x)**2 * score\n",
    "    if replace_conditioned:\n",
    "        f = f * (1-condition_mask[:len(node_ids)])\n",
    "    \n",
    "    return f\n",
    "\n",
    "# Reverse SDE diffusion\n",
    "def diffusion_backward(t,x, node_ids=node_ids,condition_mask=condition_mask, replace_conditioned=True):\n",
    "    b =  sde.diffusion(t,x) \n",
    "    if replace_conditioned:\n",
    "        b = b * (1-condition_mask[:len(node_ids)])\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "end_std = jnp.squeeze(sde.marginal_stddev(jnp.ones(1)))\n",
    "end_mean = jnp.squeeze(sde.marginal_mean(jnp.ones(1)))\n",
    "\n",
    "@partial(jax.jit, static_argnums=(1,3,7,8))\n",
    "def sample_fn(key, shape, node_ids=node_ids, time_steps=500, condition_mask=jnp.zeros((nodes_max,), dtype=int), condition_value=jnp.zeros((nodes_max,)), edge_mask=None, score_fn = model_fn, replace_conditioned=True):\n",
    "    condition_mask = condition_mask[:len(node_ids)]\n",
    "    key1, key2 = jrandom.split(key, 2)\n",
    "    # Sample from noise distribution at time 1\n",
    "    x_T = jax.random.normal(key1, shape + (len(node_ids),)) * end_std[node_ids] + end_mean[node_ids]\n",
    "    \n",
    "    if replace_conditioned:\n",
    "        x_T = x_T * (1-condition_mask) + condition_value * condition_mask\n",
    "    # Sove backward sde\n",
    "    keys = jrandom.split(key2, shape)\n",
    "    ys = jax.vmap(lambda *args: sdeint(*args, noise_type=\"diagonal\"), in_axes= (0, None, None, 0, None), out_axes=0)(keys, lambda t, x: drift_backward(t, x, node_ids, condition_mask, edge_mask=edge_mask, score_fn=score_fn, replace_conditioned=replace_conditioned), lambda t, x: diffusion_backward(t, x, node_ids, condition_mask, replace_conditioned=replace_conditioned), x_T, jnp.linspace(1.,T_min, time_steps))\n",
    "    return ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full joint estimation\n",
    "samples = sample_fn(jrandom.PRNGKey(0), (10000,), node_ids, condition_mask=jnp.zeros((nodes_max,), dtype=int), condition_value=jnp.zeros((nodes_max,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig,axes = pairplot(np.array(samples[:,-1,:]), figsize=(5,5), labels=[\"$\\\\theta_1$\", \"$x_1$\", \"$x_2$\"], diag_kind=\"kde\", color=\"black\", linewidth=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marginal estimation alone (if you leave out marginal consitstency in the loss, this will fail but the above will still work!)\n",
    "marginal_samples1  = sample_fn(jrandom.PRNGKey(0), (10000,), jnp.array([0,]), condition_mask=jnp.zeros((1,), dtype=int), condition_value=jnp.zeros((1,)))\n",
    "marginal_samples2  = sample_fn(jrandom.PRNGKey(0), (10000,), jnp.array([1,]), condition_mask=jnp.zeros((1,), dtype=int), condition_value=jnp.zeros((1,)))\n",
    "marginal_samples3 = sample_fn(jrandom.PRNGKey(0), (10000,), jnp.array([2,]), condition_mask=jnp.zeros((1,), dtype=int), condition_value=jnp.zeros((1,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(5,2))\n",
    "sns.histplot(marginal_samples1[:, -1, 0], bins=50, ax=ax[0], stat=\"density\")\n",
    "sns.histplot(marginal_samples2[:, -1, 0], bins=50, ax=ax[1], stat=\"density\")\n",
    "sns.histplot(marginal_samples3[:, -1, 0], bins=50, ax=ax[2], stat=\"density\")\n",
    "sns.kdeplot(data[:, 0, 0], ax=ax[0], color=\"black\", linewidth=2)\n",
    "sns.kdeplot(data[:, 1, 0], ax=ax[1], color=\"black\", linewidth=2)\n",
    "sns.kdeplot(data[:, 2, 0], ax=ax[2], color=\"black\", linewidth=2)\n",
    "\n",
    "ax[0].set_xlim(-10,10)\n",
    "ax[1].set_xlim(-5,5)\n",
    "ax[2].set_xlim(-5,5)\n",
    "fig.suptitle(\"Correct individual marginal estimation\")\n",
    "for ax in ax:\n",
    "    ax.yaxis.set_visible(False)\n",
    "    # remove y spines\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Arbitrary conditional distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_posterior(theta, x1=None, x2=None):\n",
    "    potential_theta = partial(log_potential, x1=x1, x2=x2)\n",
    "    potential_post = potential_theta(theta)\n",
    "    potential_post = potential_post - potential_post.max()\n",
    "    potential_post = jnp.exp(potential_post)\n",
    "    potential_post = potential_post / jnp.trapz(potential_post, x=theta)\n",
    "    return potential_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full conditional estimation\n",
    "theta = jnp.linspace(-10, 10, 1000)\n",
    "x_o = data[4,:,0]\n",
    "true_post = true_posterior(theta, x_o[1], x_o[2])\n",
    "samples = sample_fn(jrandom.PRNGKey(0), (10000,), node_ids, condition_mask=jnp.array([0,1,1], dtype=jnp.bool_), condition_value=x_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_theta = samples[...,-1, 0]\n",
    "\n",
    "fig = plt.figure(figsize=(5, 2))\n",
    "sns.histplot(samples_theta, bins=50, stat=\"density\")\n",
    "plt.plot(theta, true_post, color=\"black\", label=\"True posterior\")\n",
    "exact_patch = mpatches.Patch(color='black', label='Exact')\n",
    "approx_patch = mpatches.Patch(color='C0', label='Approx.')\n",
    "plt.legend(handles=[exact_patch, approx_patch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial conditional estimation\n",
    "theta = jnp.linspace(-10, 10, 1000)\n",
    "x_o = data[4,:,0]\n",
    "true_post = true_posterior(theta, x_o[1], None)\n",
    "samples = sample_fn(jrandom.PRNGKey(0), (10000,), node_ids, condition_mask=jnp.array([0,1,0], dtype=int), condition_value=x_o, time_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_theta = samples[...,-1, 0]\n",
    "\n",
    "fig = plt.figure(figsize=(5, 2))\n",
    "sns.histplot(samples_theta, bins=80, stat=\"density\")\n",
    "plt.plot(theta, true_post, color=\"black\", label=\"True posterior\")\n",
    "exact_patch = mpatches.Patch(color='black', label='Exact')\n",
    "approx_patch = mpatches.Patch(color='C0', label='Approx.')\n",
    "plt.legend(handles=[exact_patch, approx_patch])\n",
    "plt.xlim(-10, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Arbitrary constraints\n",
    "\n",
    "Here a simplified version constraint conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified sampling with guidance...\n",
    "\n",
    "def s(t):\n",
    "    # Some scaling function\n",
    "    t = jnp.atleast_1d(t)\n",
    "    return jnp.exp(-t**2*5)*100\n",
    "\n",
    "def log_step_fun(t,x,a,b):\n",
    "    # Step_fn constraint -> Intervals\n",
    "    scale = s(t)\n",
    "    x1 = jax.nn.log_sigmoid(jnp.sum(scale * (x - a), axis=-1))\n",
    "    x2 = jax.nn.log_sigmoid(jnp.sum(-scale * (x - b), axis=-1))\n",
    "\n",
    "    return x1 + x2\n",
    "\n",
    "def log_linear_fn_approximation(t,x, a):\n",
    "    # Linear constrait <s,a> = 0.\n",
    "    scale = s(t)\n",
    "    a = a.reshape(-1,x.shape[1],1)\n",
    "    x1 = jax.nn.log_sigmoid(scale *jnp.sum((x * a), axis=1))\n",
    "    x2 = jax.nn.log_sigmoid(-scale *jnp.sum((x * a), axis=1))\n",
    "    return x1 + x2\n",
    "\n",
    "def log_polytope_fn_approximation(t,x, A):\n",
    "    # Polytope constraint \n",
    "    scale = s(t)\n",
    "    a = x.reshape(-1, x.shape[1])\n",
    "    constraint = jax.nn.relu(scale * (jnp.matmul(a, A.T) - 1.)).max(axis=-1)\n",
    "    constraint = jax.nn.log_sigmoid(-constraint) \n",
    "    return constraint\n",
    "\n",
    "\n",
    "\n",
    "step_fn_score = jax.grad(lambda *args: log_step_fun(*args).sum(), argnums=1)\n",
    "linear_fn_score = jax.grad(lambda *args: log_linear_fn_approximation(*args).sum(), argnums=1)\n",
    "polytope_fn_score = jax.grad(lambda *args: log_polytope_fn_approximation(*args).sum(), argnums=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wappers around score functions to include the constraints\n",
    "\n",
    "def interval_score(params, t, x, node_ids, condition_mask, edge_mask, a,b):\n",
    "    condition_mask = condition_mask[:len(node_ids)].reshape(-1,len(node_ids), 1)\n",
    "    score = model_fn(params,t, x, node_ids=node_ids, condition_mask=jnp.zeros_like(condition_mask), edge_mask=edge_mask)\n",
    "    tweedies_x0_estimator = (x + sde.marginal_stddev(t, jnp.array([1.]))**2 * score)/sde.marginal_mean(t, jnp.array([1.])) # Predict x0\n",
    "    interval_score_est = step_fn_score(t,tweedies_x0_estimator,a,b).reshape(-1, len(node_ids), 1) * condition_mask.reshape(-1, len(node_ids), 1)\n",
    "    return score + interval_score_est\n",
    "\n",
    "def linear_score(params, t, x, node_ids, condition_mask, edge_mask, a):\n",
    "    condition_mask = condition_mask[:len(node_ids)].reshape(-1,len(node_ids), 1)\n",
    "    score = model_fn(params,t, x, node_ids=node_ids, condition_mask=jnp.zeros_like(condition_mask), edge_mask=edge_mask)\n",
    "    tweedies_x0_estimator = (x + sde.marginal_stddev(t, jnp.array([1.]))**2 * score)/sde.marginal_mean(t, jnp.array([1.])) # Predict x0\n",
    "    linear_score_est = linear_fn_score(t,tweedies_x0_estimator,a) * condition_mask.reshape(-1, len(node_ids), 1)\n",
    "    return score + linear_score_est\n",
    "\n",
    "def polytope_score(params, t, x,  node_ids, condition_mask, edge_mask, A):\n",
    "    condition_mask = condition_mask[:len(node_ids)].reshape(-1,len(node_ids), 1)\n",
    "    score = model_fn(params,t, x, node_ids=node_ids, condition_mask=jnp.zeros_like(condition_mask), edge_mask=edge_mask)\n",
    "    tweedies_x0_estimator = (x + sde.marginal_stddev(t, jnp.array([1.]))**2 * score)/sde.marginal_mean(t, jnp.array([1.])) # Predict x0\n",
    "    polytope_score_est = polytope_fn_score(t,tweedies_x0_estimator,A) #* condition_mask.reshape(-1, len(node_ids), 1)\n",
    "    return score + polytope_score_est\n",
    "\n",
    "score_fn1 = partial(interval_score, a=2.*jnp.ones(1), b=jnp.ones(1)*3)\n",
    "score_fn3 = partial(linear_score, a=jnp.array([1., 5.]))\n",
    "score_fn4 = partial(polytope_score, A=0.3*jax.random.normal(jrandom.PRNGKey(0), (8,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samples1 = sample_fn(jrandom.PRNGKey(0), (1000,), time_steps=1000, node_ids=jnp.array([0,1]), condition_mask=jnp.array([0,1], dtype=int), score_fn=score_fn1, replace_conditioned=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samples2 = sample_fn(jrandom.PRNGKey(0), (1000,), time_steps=1000, node_ids=jnp.array([0,1]), condition_mask=jnp.array([1,1], dtype=int), score_fn=score_fn3, replace_conditioned=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples3 = sample_fn(jrandom.PRNGKey(0), (1000,), time_steps=1000, node_ids=jnp.array([0,1]), condition_mask=jnp.array([1,1], dtype=int), score_fn=score_fn4, replace_conditioned=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliarly for plotting\n",
    "with jax.default_device(jax.devices(\"cpu\")[0]):\n",
    "    def pot(x):\n",
    "        return jnp.exp(log_polytope_fn_approximation(0.001, x, 0.3*jax.random.normal(jrandom.PRNGKey(0), (8,2))))\n",
    "\n",
    "\n",
    "    potential_fn = jax.vmap(jax.scipy.stats.gaussian_kde(data[:,[0,1], 0].T, bw_method=\"silverman\"))\n",
    "    x = jnp.linspace(data[:, 0, 0].min(), data[:, 0, 0].max(), 200)\n",
    "    y = jnp.linspace(data[:, 1, 0].min(), data[:, 1, 0].max(), 200)\n",
    "    X, Y = jnp.meshgrid(x, y)\n",
    "    pos = jnp.dstack((X, Y))\n",
    "    Z_true = potential_fn(pos.reshape(-1,2)).reshape(200,200)\n",
    "    Z = pot(pos.reshape(-1,2)).reshape(200,200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scoresbibm.plot import use_style\n",
    "fig, axs = plt.subplots(2, 1, figsize=(4., 4.5), gridspec_kw={'height_ratios': [1, 3]})\n",
    "\n",
    "# 2D scatter plot\n",
    "l = axs[1].contour(X, Y, Z,colors=\"tab:red\", levels=[0.4], alpha=0.5, linewidths=3)\n",
    "axs[1].contourf(X, Y, Z_true, cmap=\"Greys\", levels=200, vmin=0, alpha=0.9)\n",
    "axs[1].scatter(samples1[:, -1, 0], samples1[:, -1, 1], s=1, alpha=0.2, color=\"tab:green\")\n",
    "axs[1].scatter(samples2[:, -1, 0], samples2[:, -1, 1], s=1, alpha=0.05, color=\"tab:blue\")\n",
    "axs[1].scatter(samples3[:, -1, 0], samples3[:, -1, 1], s=1, alpha=0.2, color=\"tab:red\")\n",
    "axs[1].set_xlim(-10, 10)\n",
    "axs[1].set_ylim(-5, 5)\n",
    "axs[1].set_xlabel(r\"$\\theta$\")\n",
    "axs[1].set_ylabel(r\"$x_1$\")\n",
    "\n",
    "# 1D marginal plot\n",
    "_ = sns.histplot(samples1[:, -1, 0], ax=axs[0], bins=20, color=\"tab:green\", stat=\"density\")\n",
    "_ = sns.histplot(samples2[:, -1, 0], ax=axs[0], bins=20,color=\"tab:blue\", stat=\"density\")\n",
    "_ = sns.histplot(samples3[:, -1, 0], ax=axs[0],bins=20, color=\"tab:red\", stat=\"density\")\n",
    "axs[0].set_xlim(-10, 10)\n",
    "axs[0].set_xticklabels([])\n",
    "axs[0].set_yticklabels([])\n",
    "axs[0].set_ylabel(\"\")\n",
    "axs[0].yaxis.set_visible(False)\n",
    "axs[1].set_xticks([-10,0,10])\n",
    "axs[1].set_yticks([-5,0,5])\n",
    "\n",
    "axs[0].legend([r\"$p(\\theta|x_1 \\in [2,3])$\", r\"$p(\\theta|x_1 + \\theta = 0)$\", r\"$p(\\theta|(x_1,\\theta)\\in S)$\"], ncol=3, loc=\"upper center\", bbox_to_anchor=(0.5, 1.5),columnspacing=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Change priors or likelihoods\n",
    "\n",
    "\n",
    "$$ p(\\theta) \\propto \\exp\\left(-\\frac{(\\theta - \\mu)^2}{2\\sigma^2}\\right) $$\n",
    "$$ \\nabla_\\theta \\log p(\\theta) = \\frac{\\theta - \\mu}{\\sigma^2} $$\n",
    "\n",
    "In training we have $\\sigma^2 = 9$.\n",
    "Now for any $\\alpha$ we have\n",
    "$$ p(\\theta)^\\alpha = \\exp\\left(-\\frac{\\alpha(\\theta - \\mu)^2}{2\\sigma^2}\\right) = \\exp\\left(-\\frac{(\\theta - \\mu)^2}{2\\sigma^2/\\alpha}\\right) $$\n",
    "\n",
    "So if we want a certain variance $\\sigma_0^2$. All we have to do is to multiply the score by $\\alpha = \\sigma^2/\\sigma_0^2$.\n",
    "\n",
    "If we want to change the mean then we have to add $\\mu_0 - \\mu / \\sigma^2$ to the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score function wrappers with manipulations\n",
    "def score_prior_change(params, t, x, node_ids, condition_mask, edge_mask, node_ids_to_change, scale=1., shift=0.):\n",
    "\n",
    "    index = jnp.searchsorted(node_ids,node_ids_to_change)    \n",
    "    score_unconditioned = model_fn(params, t, x[:,index], node_ids=node_ids_to_change, condition_mask=jnp.zeros_like(node_ids_to_change))\n",
    "\n",
    "    score_conditional = model_fn(params, t, x, node_ids=node_ids, condition_mask=condition_mask, edge_mask=edge_mask)\n",
    "    score_conditional = score_conditional.at[:,index].set(scale * (score_unconditioned - shift) + (score_conditional[:,index] - score_unconditioned))\n",
    "    \n",
    "    return score_conditional\n",
    "\n",
    "def score_likelihood_change(params, t, x, node_ids, condition_mask, edge_mask, node_ids_to_change, scale=1., shift=0.):\n",
    "\n",
    "    index = jnp.searchsorted(node_ids,node_ids_to_change)    \n",
    "    score_unconditioned = model_fn(params, t, x[:,index], node_ids=node_ids_to_change, condition_mask=jnp.zeros_like(node_ids_to_change))\n",
    "\n",
    "    score_conditional = model_fn(params, t, x, node_ids=node_ids, condition_mask=condition_mask, edge_mask=edge_mask)\n",
    "    score_conditional =  score_conditional.at[:,index].set(score_unconditioned + (scale*(score_conditional[:,index] - score_unconditioned)) - shift)\n",
    "    \n",
    "    return score_conditional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.linspace(-10, 10, 1000)\n",
    "potential_theta = partial(log_potential, x1=jnp.zeros((1,)), x2=None, mean_scale=jnp.sqrt(12.))\n",
    "potential_post = potential_theta(x)\n",
    "potential_post = potential_post - potential_post.max()\n",
    "potential_post = jnp.exp(potential_post)\n",
    "potential_post = potential_post / jnp.trapz(potential_post, x=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to N(0, 2) prior -> shift = 9/4\n",
    "score_fn_prior_change = partial(score_prior_change, scale=9/12., shift=0., node_ids_to_change=jnp.array([0]))\n",
    "samples = sample_fn(jrandom.PRNGKey(0), (5000,), time_steps=500, node_ids=jnp.array([0,1]), condition_mask=jnp.array([0,1], dtype=int), condition_value=jnp.zeros((2,)),  edge_mask=None, score_fn =score_fn_prior_change)\n",
    "\n",
    "fig = plt.figure(figsize=(3, 2))\n",
    "ax = plt.gca()\n",
    "sns.histplot(samples[:, -1, 0], bins=100, stat=\"density\", label= \"Approx.\")\n",
    "plt.plot(x, jax.scipy.stats.norm.pdf(x, 0, 2), color=\"black\", linestyle=\"--\", label=\"Prior\")\n",
    "plt.plot(x, potential_post, color=\"black\", label=\"Exact\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(r\"$\\theta$\")\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.set_xticks([-10,0,10])   \n",
    "ax.set_xlim(-10,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.linspace(-10, 10, 1000)\n",
    "potential_theta = partial(log_potential, x1=jnp.zeros((1,)), x2=None, mean_scale=1, mean_loc=2)\n",
    "potential_post = potential_theta(x)\n",
    "potential_post = potential_post - potential_post.max()\n",
    "potential_post = jnp.exp(potential_post)\n",
    "potential_post = potential_post / jnp.trapz(potential_post, x=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to N(2, 1) prior -> shift = 9/2, loc = 2/9\n",
    "score_fn_prior_change = partial(score_prior_change, scale=9/2, shift=-2/9, node_ids_to_change=jnp.array([0]))\n",
    "samples = sample_fn(jrandom.PRNGKey(0), (2000,), time_steps=500, node_ids=jnp.array([0,1]), condition_mask=jnp.array([0,1], dtype=int), condition_value=jnp.zeros((2,)),  edge_mask=None, score_fn =score_fn_prior_change)\n",
    "\n",
    "fig = plt.figure(figsize=(3, 2))\n",
    "ax = plt.gca()\n",
    "sns.histplot(samples[:, -1, 0], bins=50, stat=\"density\", label= \"Approx.\")\n",
    "plt.plot(x, jax.scipy.stats.norm.pdf(x, 2., np.sqrt(2.)), color=\"black\", linestyle=\"--\", label=\"Prior\")\n",
    "plt.plot(x, potential_post, color=\"black\", label=\"Exact\")\n",
    "plt.legend()\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.set_xticks([-10,0,10])   \n",
    "plt.xlabel(r\"$\\theta$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.linspace(-10, 10, 1000)\n",
    "potential_theta = partial(log_potential, x1=jnp.zeros((1,)), x2=None, sigma_x1=jnp.sqrt(0.05))\n",
    "potential_post = potential_theta(x)\n",
    "potential_post = potential_post - potential_post.max()\n",
    "potential_post = jnp.exp(potential_post)\n",
    "potential_post = potential_post / jnp.trapz(potential_post, x=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood very certain\n",
    "score_fn_likelihood_change = partial(score_likelihood_change, scale=0.5**2/0.05, shift=0., node_ids_to_change=jnp.array([0]))\n",
    "samples = sample_fn(jrandom.PRNGKey(0), (2000,), time_steps=500, node_ids=jnp.array([0,1]), condition_mask=jnp.array([0,1], dtype=int), condition_value=jnp.zeros((2,)),  edge_mask=None, score_fn =score_fn_likelihood_change)\n",
    "\n",
    "fig = plt.figure(figsize=(3, 2))\n",
    "ax = plt.gca()\n",
    "sns.histplot(samples[:, -1, 0], bins=200, stat=\"density\")\n",
    "plt.plot(x, potential_post, color=\"black\")\n",
    "plt.legend([\"exact\", \"approx.\"], loc=\"upper left\")\n",
    "plt.xlim(-10,10)\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.set_xticks([-10,0,10])   \n",
    "plt.xlabel(r\"$\\theta$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.linspace(-10, 10, 1000)\n",
    "potential_theta = partial(log_potential, x1=jnp.zeros((1,)), x2=None, sigma_x1=2.)\n",
    "potential_post = potential_theta(x)\n",
    "potential_post = potential_post - potential_post.max()\n",
    "potential_post = jnp.exp(potential_post)\n",
    "potential_post = potential_post / jnp.trapz(potential_post, x=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood very certain\n",
    "score_fn_likelihood_change = partial(score_likelihood_change, scale=0.5**2/4., shift=0., node_ids_to_change=jnp.array([0]))\n",
    "samples = sample_fn(jrandom.PRNGKey(0), (10000,), time_steps=500, node_ids=jnp.array([0,1]), condition_mask=jnp.array([0,1], dtype=int), condition_value=jnp.zeros((2,)),  edge_mask=None, score_fn =score_fn_likelihood_change)\n",
    "\n",
    "fig = plt.figure(figsize=(3, 2))\n",
    "ax = plt.gca()\n",
    "sns.histplot(samples[:, -1, 0], bins=100, stat=\"density\")\n",
    "plt.plot(x, potential_post, color=\"black\")\n",
    "plt.legend([\"exact\", \"approx.\"], loc=\"upper left\")\n",
    "plt.xlim(-10,10)\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.set_xticks([-10,0,10])   \n",
    "plt.xlabel(r\"$\\theta$\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "simformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
